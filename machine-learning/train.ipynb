{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import azureml\r\n",
        "from azureml.core import Workspace, Dataset\r\n",
        "\r\n",
        "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611270840237
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inicjacja przestrzeni roboczej"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\r\n",
        "print('Workspace name: ' + ws.name, \r\n",
        "      'Azure region: ' + ws.location, \r\n",
        "      'Subscription id: ' + ws.subscription_id, \r\n",
        "      'Resource group: ' + ws.resource_group, sep='\\n')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611270840955
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utworzenie eksperymentu Azure ML"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\r\n",
        "\r\n",
        "script_folder = './scripts'\r\n",
        "os.makedirs(script_folder, exist_ok=True)\r\n",
        "\r\n",
        "retrain_folder = './retrain'\r\n",
        "\r\n",
        "exp = Experiment(workspace=ws, name='textgen')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611270841113
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datastore = ws.get_default_datastore()\r\n",
        "dataset = Dataset.File.from_files(path=(datastore, 'Corpus/01-17-2021_104148_UTC/'))\r\n",
        "\r\n",
        "mount_ctx = dataset.mount()  \r\n",
        "mount_ctx.start() \r\n",
        "\r\n",
        "dataset_mount_folder = mount_ctx.mount_point\r\n",
        "print(dataset_mount_folder)\r\n",
        "\r\n",
        "files = os.listdir(dataset_mount_folder)\r\n",
        "print(files)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmpkfphc6ld\n",
            "['corpus_text_12000000.txt', 'corpus_text_16000000.txt', 'corpus_text_20000000.txt', 'corpus_text_24000000.txt', 'corpus_text_28000000.txt', 'corpus_text_32000000.txt', 'corpus_text_36000000.txt', 'corpus_text_4000000.txt', 'corpus_text_40000000.txt', 'corpus_text_8000000.txt']\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611270856482
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utworzenie środowiska obliczeniowego"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "cluster_name = \"ml-gpu-cl-lp\"\r\n",
        "\r\n",
        "try:\r\n",
        "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
        "    print('Found existing compute target')\r\n",
        "except ComputeTargetException:\r\n",
        "    print('Creating a new compute target...')\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \r\n",
        "                                                           max_nodes=4)\r\n",
        "\r\n",
        "    # create the cluster\r\n",
        "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
        "\r\n",
        "# can poll for a minimum number of nodes and for a specific timeout. \r\n",
        "# if no min node count is provided it uses the scale settings for the cluster\r\n",
        "compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\r\n",
        "\r\n",
        "# use get_status() to get a detailed status for the current cluster. \r\n",
        "print(compute_target.get_status().serialize())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing compute target\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n",
            "{'currentNodeCount': 1, 'targetNodeCount': 1, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 1, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-01-21T23:04:56.776000+00:00', 'errors': None, 'creationTime': '2021-01-21T13:20:48.526839+00:00', 'modifiedTime': '2021-01-21T13:21:04.170071+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 1, 'nodeIdleTimeBeforeScaleDown': 'PT1200S'}, 'vmPriority': 'LowPriority', 'vmSize': 'STANDARD_NC6'}\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611270856764
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_targets = ws.compute_targets\r\n",
        "for name, ct in compute_targets.items():\r\n",
        "    print(name, ct.type, ct.provisioning_state)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ml-cpu ComputeInstance Succeeded\n",
            "ml-gpu-cl AmlCompute Succeeded\n",
            "ml-gpu-cl-lp AmlCompute Succeeded\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611270857054
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kopiowanie danych treningowych do skryptu"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\r\n",
        "\r\n",
        "shutil.copy('./script.py', script_folder)\r\n",
        "shutil.copy('./model/model', retrain_folder)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "'./scripts/script.py'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611271749762
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(script_folder, './script.py'), 'r') as f:\r\n",
        "    print(f.read())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import tensorflow as tf\n",
            "from tensorflow.keras.layers.experimental import preprocessing\n",
            "from tensorflow.keras.models import Sequential\n",
            "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
            "from keras.callbacks import Callback\n",
            "\n",
            "from azureml.core import Run\n",
            "\n",
            "import numpy as np\n",
            "import os\n",
            "import time\n",
            "import glob\n",
            "import string\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "\n",
            "parser = argparse.ArgumentParser()\n",
            "parser.add_argument('--data-folder', type=str, dest='data_folder', default='data', help='data folder mounting point')\n",
            "parser.add_argument('--model-folder', type=str, dest='model_folder', default=None, help='model to train')\n",
            "parser.add_argument('--text-id', type=int, dest='text_id', default=1)\n",
            "parser.add_argument('--text-size', type=int, dest='text_size', default=10000)\n",
            "parser.add_argument('--epochs', type=int, dest='epochs', default=1)\n",
            "args = parser.parse_args()\n",
            "\n",
            "data_folder = args.data_folder\n",
            "print('training dataset is stored here:', data_folder)\n",
            "files = os.listdir(data_folder)\n",
            "\n",
            "model = None\n",
            "model_folder = args.model_folder\n",
            "if model_folder is not None:\n",
            "  print('model is stored here:', model_folder)\n",
            "  model_files = os.listdir(model_folder)\n",
            "  model = tf.saved_model.load(model_folder)\n",
            "\n",
            "\n",
            "EPOCHS = args.epochs\n",
            "\n",
            "\n",
            "# Wczytanie danych\n",
            "print(\"---------- Wczytywanie danych ----------\")\n",
            "\n",
            "texts = []\n",
            "for file_ in files:\n",
            "    texts.append(open(os.path.join(data_folder, file_), 'rb').read().decode(\"utf-8\"))\n",
            "    l = (len(texts[-1]))\n",
            "    print(f\"Length of text {file_}: {l} characters\")\n",
            "\n",
            "print(\"---------- Zakończono wczytywanie danych ----------\")\n",
            "\n",
            "n = args.text_id if args.text_id < len(texts) else len(texts)\n",
            "s = args.text_size if args.text_size < len(texts[n-1]) else len(texts[n-1])\n",
            "\n",
            "texts = texts[:n]\n",
            "\n",
            "\n",
            "# Przetworzenie tekstu\n",
            "print(\"---------- Przetwarzanie tekstu 1/2----------\")\n",
            "\n",
            "for i, text in enumerate(texts):\n",
            "  text = text.lower()\n",
            "  vocab = sorted(set(text))\n",
            "  polish_alphabet = \"aąbcćdeęfghijklłmnńoóprsśtuwyzźż\"\n",
            "\n",
            "  unwanted_whitespaces = string.whitespace.translate(str.maketrans(\"\", \"\", \" \"))\n",
            "  allowed_chars = polish_alphabet + polish_alphabet.upper() + string.digits + \"!?.,:%\" + \" \"\n",
            "  unwanted_chars = \"\".join(vocab).translate(str.maketrans(\"\",\"\",allowed_chars))\n",
            "  \n",
            "  print(f\"--------- {i}/{len(texts)} ----------\")\n",
            "  text = text.translate(str.maketrans(unwanted_whitespaces, \"\".join([\" \" for _ in range(len(unwanted_whitespaces))])))\n",
            "  text = text.translate(str.maketrans(\"\", \"\", unwanted_chars))\n",
            "\n",
            "\n",
            "\n",
            "# Przetworzenie tekstu\n",
            "print(\"---------- Przetwarzanie teksty 2/2----------\")\n",
            "vocab = set()\n",
            "for text in texts:\n",
            "  vocab = vocab.union(set(text))\n",
            "vocab = sorted(vocab)\n",
            "\n",
            "ids_from_chars = preprocessing.StringLookup(\n",
            "    vocabulary=list(vocab))\n",
            "\n",
            "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
            "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
            "\n",
            "def text_from_ids(ids):\n",
            "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
            "\n",
            "\n",
            "\n",
            "print(\"---------- Tokenizacja tekstu ----------\")\n",
            "\n",
            "all_ids = []\n",
            "ids_datasets = []\n",
            "for i, text in enumerate(texts):\n",
            "  print(f\"Tokenizacja tekstu {i}/{len(texts)}\")\n",
            "  all_ids.append(ids_from_chars(tf.strings.unicode_split(text, 'UTF-8')))\n",
            "  ids_datasets.append(tf.data.Dataset.from_tensor_slices(all_ids[-1]))\n",
            "\n",
            "for ids in ids_datasets[0].take(10):\n",
            "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
            "\n",
            "print(\"---------- Tokenizacja tekstu zakończona ----------\")\n",
            "\n",
            "\n",
            "\n",
            "print(\"---------- Tworzenie zbiorów sekwencji ----------\")\n",
            "seq_length = 100\n",
            "examples_per_epoch = len(texts[0])//(seq_length+1)\n",
            "sequences_d = []\n",
            "for i, ids_dataset in enumerate(ids_datasets):\n",
            "  print(f\"Tworzenie zbiorów sekwencji {i}/{len(ids_datasets)}\")\n",
            "  sequences_d.append(ids_dataset.batch(seq_length+1, drop_remainder=True))\n",
            "\n",
            "for seq in sequences_d[0].take(5):\n",
            "  print((text_from_ids(seq).numpy()).decode(\"utf-8\"))\n",
            "\n",
            "print(\"---------- Tworzenie zbiorów wejść i wyjść ----------\")\n",
            "def split_input_target(sequence):\n",
            "    input_text = sequence[:-1]\n",
            "    target_text = sequence[1:]\n",
            "    return input_text, target_text\n",
            "\n",
            "split_input_target(list(\"Korpus parlamentarny\"))\n",
            "\n",
            "datasets = []\n",
            "for sequences in enumerate(sequences_d):\n",
            "  print(f\"{i}/{len(sequences_d)}\")\n",
            "  datasets.append(sequences_d[i].map(split_input_target))\n",
            "\n",
            "for input_example, target_example in  datasets[0].take(2):\n",
            "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
            "    print(\"Target:\", text_from_ids(target_example).numpy())\n",
            "\n",
            "\n",
            "print(\"---------- Tworzenie paczek szkoleniowych ----------\")\n",
            "# Tworzenie paczek szkoleniowych\n",
            "# Batch size\n",
            "BATCH_SIZE = 64\n",
            "\n",
            "# Buffer size to shuffle the dataset\n",
            "# (TF data is designed to work with possibly infinite sequences,\n",
            "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
            "# it maintains a buffer in which it shuffles elements).\n",
            "BUFFER_SIZE = 10000\n",
            "\n",
            "dataset_train = datasets[0]\n",
            "for dataset in datasets[1:]:\n",
            "  dataset_train = dataset_train.concatenate(dataset)\n",
            "\n",
            "dataset_train = (\n",
            "    dataset_train\n",
            "    .shuffle(BUFFER_SIZE)\n",
            "    .batch(BATCH_SIZE, drop_remainder=True)\n",
            "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
            "\n",
            "\n",
            "\n",
            "# Budowanie modelu\n",
            "print(\"---------- Budowanie modelu ---------\")\n",
            "\n",
            "vocab_size = len(vocab)\n",
            "\n",
            "embedding_dim = 256\n",
            "\n",
            "units = 1024\n",
            "\n",
            "\n",
            "class MyModelRNN(tf.keras.Model):\n",
            "  def __init__(self, vocab_size, embedding_dim, units):\n",
            "    super().__init__(self)\n",
            "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
            "    self.gru = tf.keras.layers.GRU(units,\n",
            "                                   return_sequences=True, \n",
            "                                   return_state=True)\n",
            "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
            "\n",
            "  def call(self, inputs, states=None, return_state=False, training=False):\n",
            "    x = inputs\n",
            "    x = self.embedding(x, training=training)\n",
            "    if states is None:\n",
            "      states = self.gru.get_initial_state(x)\n",
            "    x, states = self.gru(x, initial_state=states, training=training)\n",
            "    x = self.dense(x, training=training)\n",
            "\n",
            "    if return_state:\n",
            "      return x, states\n",
            "    else: \n",
            "      return x\n",
            "\n",
            "\n",
            "if model is None:\n",
            "  model = MyModelRNN(\n",
            "      vocab_size=len(ids_from_chars.get_vocabulary()),\n",
            "      embedding_dim=embedding_dim,\n",
            "      units=units)\n",
            "\n",
            "for input_example_batch, target_example_batch in dataset_train.take(1):\n",
            "    example_batch_predictions = model(input_example_batch)\n",
            "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
            "\n",
            "model.summary()\n",
            "\n",
            "\n",
            "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
            "\n",
            "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
            "mean_loss = example_batch_loss.numpy().mean()\n",
            "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
            "print(\"Mean loss:        \", mean_loss)\n",
            "\n",
            "\n",
            "tf.exp(mean_loss).numpy()\n",
            "\n",
            "\n",
            "model.compile(optimizer='adam', loss=loss)\n",
            "\n",
            "# start an Azure ML run\n",
            "run = Run.get_context()\n",
            "\n",
            "class LogRunMetrics(Callback):\n",
            "    def on_epoch_end(self, epoch, log):\n",
            "        run.log('Loss', log['loss'])\n",
            "\n",
            "# Directory where the checkpoints will be saved\n",
            "checkpoint_dir = './outputs/training_checkpoints'\n",
            "# Name of the checkpoint files\n",
            "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
            "\n",
            "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
            "    filepath=checkpoint_prefix,\n",
            "    save_weights_only=True)\n",
            "\n",
            "\n",
            "\n",
            "print(\"---------- Trenowanie modelu ----------\")\n",
            "\n",
            "history = model.fit(dataset_train, epochs=EPOCHS, verbose=1, callbacks=[LogRunMetrics(), checkpoint_callback])\n",
            "\n",
            "\n",
            "# log a single value\n",
            "plt.figure(figsize=(6, 3))\n",
            "plt.title('TextGen with Keras ({} epochs)'.format(EPOCHS), fontsize=14)\n",
            "plt.plot(history.history['loss'], 'r--', label='Loss', lw=4, alpha=0.5)\n",
            "plt.legend(fontsize=12)\n",
            "plt.grid(True)\n",
            "\n",
            "# log an image\n",
            "run.log_image('Loss', plot=plt)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "print(\"---------- Zapis modelu ----------\")\n",
            "# create a ./outputs/model folder in the compute target\n",
            "# files saved in the \"./outputs\" folder are automatically uploaded into run history\n",
            "os.makedirs('./outputs/model', exist_ok=True)\n",
            "tf.saved_model.save(model, './outputs/model/model')\n",
            "print(\"model saved in ./outputs/model folder\")\n",
            "\n",
            "\n",
            "\n",
            "# Generowanie tekstu\n",
            "print(\"---------- Tworzenie modelu generującego znak ---------\")\n",
            "\n",
            "class OneStep(tf.keras.Model):\n",
            "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1):\n",
            "    super().__init__()\n",
            "    self.temperature=temperature\n",
            "    self.model = model\n",
            "    self.chars_from_ids = chars_from_ids\n",
            "    self.ids_from_chars = ids_from_chars\n",
            "\n",
            "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
            "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
            "    sparse_mask = tf.SparseTensor(\n",
            "        # Put a -inf at each bad index.\n",
            "        values=[-float('inf')]*len(skip_ids),\n",
            "        indices = skip_ids,\n",
            "        # Match the shape to the vocabulary\n",
            "        dense_shape=[len(ids_from_chars.get_vocabulary())]) \n",
            "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
            "\n",
            "  @tf.function\n",
            "  def generate_one_step(self, inputs, states=None):\n",
            "    # Convert strings to token IDs.\n",
            "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
            "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
            "\n",
            "    # Run the model.\n",
            "    # predicted_logits.shape is [batch, char, next_char_logits] \n",
            "    predicted_logits, states =  self.model(inputs=input_ids, states=states, \n",
            "                                          return_state=True)\n",
            "    # Only use the last prediction.\n",
            "    predicted_logits = predicted_logits[:, -1, :]\n",
            "    predicted_logits = predicted_logits/self.temperature\n",
            "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
            "    predicted_logits = predicted_logits + self.prediction_mask\n",
            "\n",
            "    # Sample the output logits to generate token IDs.\n",
            "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
            "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
            "\n",
            "    # Convert from token ids to characters\n",
            "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
            "\n",
            "    # Return the characters and model state.\n",
            "    return predicted_chars, states\n",
            "\n",
            "\n",
            "\n",
            "print(\"--------- Tworzenie modelu generującego tekst ----------\")\n",
            "\n",
            "class TextGenerator(OneStep):\n",
            "    def __init__(self, model, chars_from_ids, ids_from, temperature=1.0):\n",
            "        super().__init__(model, chars_from_ids, ids_from, 1.0)\n",
            "\n",
            "\n",
            "    def generate(self, next_char='', n_chars=100):\n",
            "        states = None\n",
            "        next_char = tf.constant([next_char])\n",
            "        result = [next_char]\n",
            "\n",
            "        for n in range(n_chars):\n",
            "            next_char, states = self.generate_one_step(next_char, states=states)\n",
            "            result.append(next_char)\n",
            "\n",
            "        result = tf.strings.join(result)\n",
            "        return result[0].numpy().decode(\"utf-8\")\n",
            "\n",
            "\n",
            "\n",
            "print(\"---------- Zapisywanie modelu generującego tekst ---------\")\n",
            "\n",
            "textgenerator = TextGenerator(model, chars_from_ids, ids_from_chars)\n",
            "\n",
            "# create a ./outputs/model folder in the compute target\n",
            "# files saved in the \"./outputs\" folder are automatically uploaded into run history\n",
            "tf.saved_model.save(model, './outputs/model/textgenerator')\n",
            "print(\"model saved in ./outputs/model folder\")\n",
            "\n"
          ]
        }
      ],
      "execution_count": 26,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611271750121
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utworzenie środwiska"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile conda_dependencies.yml\r\n",
        "\r\n",
        "channels:\r\n",
        "- conda-forge\r\n",
        "dependencies:\r\n",
        "- python=3.6.2\r\n",
        "- pip:\r\n",
        "  - h5py<=2.10.0\r\n",
        "  - azureml-defaults\r\n",
        "  - tensorflow-gpu\r\n",
        "  - keras\r\n",
        "  - tensorflow\r\n",
        "  - matplotlib"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting conda_dependencies.yml\n"
          ]
        }
      ],
      "execution_count": 27,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\r\n",
        "\r\n",
        "textgen_env = Environment.from_conda_specification(name = 'texgen', file_path = './conda_dependencies.yml')\r\n",
        "\r\n",
        "# Specify a GPU base image\r\n",
        "textgen_env.docker.enabled = True\r\n",
        "textgen_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.0-cudnn7-ubuntu18.04'\r\n",
        "\r\n",
        "#textgen_env = Environment.get(ws, name='AzureML-TensorFlow-2.3-GPU')"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611271764797
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Konfiguracja treningu"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\r\n",
        "model = tf.saved_model.load('./model/model')\r\n",
        "type(model)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 32,
          "data": {
            "text/plain": "tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject"
          },
          "metadata": {}
        }
      ],
      "execution_count": 32,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611271771841
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import ScriptRunConfig\r\n",
        "\r\n",
        "args = ['--data-folder', dataset.as_named_input('text').as_mount(),\r\n",
        "        '--text-id', 1,\r\n",
        "        #'--text-size', 10000,\r\n",
        "        '--epochs', 1]\r\n",
        "\r\n",
        "src = ScriptRunConfig(source_directory=script_folder,\r\n",
        "                      script='script.py',\r\n",
        "                      arguments=args,\r\n",
        "                      compute_target=compute_target,\r\n",
        "                      environment=textgen_env)"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611271771976
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zatwierdzenie zadania do uruchomienia"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = exp.submit(src)"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611271774266
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\r\n",
        "RunDetails(run).show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31a614ab57fc4cd5bf89da591a80acd9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/textgen/runs/textgen_1611271774_5faca64f?wsid=/subscriptions/fc814e44-1cd5-4ab5-944b-f2255f816d34/resourcegroups/text-generation/workspaces/deep-learning\", \"run_id\": \"textgen_1611271774_5faca64f\", \"run_properties\": {\"run_id\": \"textgen_1611271774_5faca64f\", \"created_utc\": \"2021-01-21T23:29:36.176024Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"e3dc5102-234d-44eb-a6a5-b2c076a2e9e7\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {\"_aml_system_ComputeTargetStatus\": \"{\\\"AllocationState\\\":\\\"steady\\\",\\\"PreparingNodeCount\\\":0,\\\"RunningNodeCount\\\":0,\\\"CurrentNodeCount\\\":1}\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"azureml-logs/55_azureml-execution-tvmps_cff03917453a8001f928502ec2a6147c013beeb956d6b3167d39aa64a00686f6_p.txt\": \"https://deeplearstorage827de82c2.blob.core.windows.net/azureml/ExperimentRun/dcid.textgen_1611271774_5faca64f/azureml-logs/55_azureml-execution-tvmps_cff03917453a8001f928502ec2a6147c013beeb956d6b3167d39aa64a00686f6_p.txt?sv=2019-02-02&sr=b&sig=CZwc5Jd04PEw43a1ZSm8OTcIWoK2JlKbsHWwPIQgjas%3D&st=2021-01-21T23%3A20%3A16Z&se=2021-01-22T07%3A30%3A16Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_cff03917453a8001f928502ec2a6147c013beeb956d6b3167d39aa64a00686f6_p.txt\": \"https://deeplearstorage827de82c2.blob.core.windows.net/azureml/ExperimentRun/dcid.textgen_1611271774_5faca64f/azureml-logs/65_job_prep-tvmps_cff03917453a8001f928502ec2a6147c013beeb956d6b3167d39aa64a00686f6_p.txt?sv=2019-02-02&sr=b&sig=HBzlIGbLlsQPZVMXRlPXwbHKuh7wAIQjCkQSZbr6bSU%3D&st=2021-01-21T23%3A20%3A16Z&se=2021-01-22T07%3A30%3A16Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://deeplearstorage827de82c2.blob.core.windows.net/azureml/ExperimentRun/dcid.textgen_1611271774_5faca64f/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=FX6k7o%2BhOjghsHaVCt%2BAROXjL%2BSH3g33sy2RkNIpgh4%3D&st=2021-01-21T23%3A20%3A16Z&se=2021-01-22T07%3A30%3A16Z&sp=r\", \"azureml-logs/process_info.json\": \"https://deeplearstorage827de82c2.blob.core.windows.net/azureml/ExperimentRun/dcid.textgen_1611271774_5faca64f/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=sJW2moxmWJ4UfRYC76RJ1zxK8MpkEfwp0pAEf7xZGto%3D&st=2021-01-21T23%3A20%3A16Z&se=2021-01-22T07%3A30%3A16Z&sp=r\", \"azureml-logs/process_status.json\": \"https://deeplearstorage827de82c2.blob.core.windows.net/azureml/ExperimentRun/dcid.textgen_1611271774_5faca64f/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=Zy91Q5Bp0uq4VCiUBZY0v5j0RM6MHrwrFBZSyuqVLe4%3D&st=2021-01-21T23%3A20%3A16Z&se=2021-01-22T07%3A30%3A16Z&sp=r\", \"logs/azureml/73_azureml.log\": \"https://deeplearstorage827de82c2.blob.core.windows.net/azureml/ExperimentRun/dcid.textgen_1611271774_5faca64f/logs/azureml/73_azureml.log?sv=2019-02-02&sr=b&sig=HYtjgU5OSkGt3eu3zmRruyWzZiFdztTETQJEAmNs1aw%3D&st=2021-01-21T23%3A20%3A16Z&se=2021-01-22T07%3A30%3A16Z&sp=r\", \"logs/azureml/dataprep/backgroundProcess.log\": \"https://deeplearstorage827de82c2.blob.core.windows.net/azureml/ExperimentRun/dcid.textgen_1611271774_5faca64f/logs/azureml/dataprep/backgroundProcess.log?sv=2019-02-02&sr=b&sig=hHdhxD9jlmHoRNhjM5mQAqrzt2A%2Bf7UAPQZu86ItcUI%3D&st=2021-01-21T23%3A20%3A16Z&se=2021-01-22T07%3A30%3A16Z&sp=r\", \"logs/azureml/dataprep/backgroundProcess_Telemetry.log\": \"https://deeplearstorage827de82c2.blob.core.windows.net/azureml/ExperimentRun/dcid.textgen_1611271774_5faca64f/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-02-02&sr=b&sig=HCErwafF3SDxs54%2B%2BF13TKG%2Frp5%2BoZZj%2FQPU7pXDmfA%3D&st=2021-01-21T23%3A20%3A16Z&se=2021-01-22T07%3A30%3A16Z&sp=r\", \"logs/azureml/dataprep/engine_spans_02cf2465-81c4-462e-837f-a64fc8edff3b.jsonl\": \"https://deeplearstorage827de82c2.blob.core.windows.net/azureml/ExperimentRun/dcid.textgen_1611271774_5faca64f/logs/azureml/dataprep/engine_spans_02cf2465-81c4-462e-837f-a64fc8edff3b.jsonl?sv=2019-02-02&sr=b&sig=t7d644WCLg7W%2BF8Dmc8zg0Kn9rCG70KhczcXjazj%2B5U%3D&st=2021-01-21T23%3A20%3A16Z&se=2021-01-22T07%3A30%3A16Z&sp=r\", \"logs/azureml/dataprep/engine_spans_65e52490-dfa0-4f1a-9482-e15c542a6721.jsonl\": \"https://deeplearstorage827de82c2.blob.core.windows.net/azureml/ExperimentRun/dcid.textgen_1611271774_5faca64f/logs/azureml/dataprep/engine_spans_65e52490-dfa0-4f1a-9482-e15c542a6721.jsonl?sv=2019-02-02&sr=b&sig=UOle5svMq%2FmFp3lAgYitd7qetYag8XFoWo3z3TnQLRc%3D&st=2021-01-21T23%3A20%3A16Z&se=2021-01-22T07%3A30%3A16Z&sp=r\", \"logs/azureml/dataprep/python_span_02cf2465-81c4-462e-837f-a64fc8edff3b.jsonl\": \"https://deeplearstorage827de82c2.blob.core.windows.net/azureml/ExperimentRun/dcid.textgen_1611271774_5faca64f/logs/azureml/dataprep/python_span_02cf2465-81c4-462e-837f-a64fc8edff3b.jsonl?sv=2019-02-02&sr=b&sig=V5ABTWgUwG0LflW9LjI3LqheEHXrzlB1K%2FlhVDCAWhY%3D&st=2021-01-21T23%3A20%3A16Z&se=2021-01-22T07%3A30%3A16Z&sp=r\", \"logs/azureml/dataprep/python_span_65e52490-dfa0-4f1a-9482-e15c542a6721.jsonl\": \"https://deeplearstorage827de82c2.blob.core.windows.net/azureml/ExperimentRun/dcid.textgen_1611271774_5faca64f/logs/azureml/dataprep/python_span_65e52490-dfa0-4f1a-9482-e15c542a6721.jsonl?sv=2019-02-02&sr=b&sig=MLF83lHV8PXOYk90AF6yhdS2GN1XimY5UC5Ni0aVWw0%3D&st=2021-01-21T23%3A20%3A16Z&se=2021-01-22T07%3A30%3A16Z&sp=r\", \"logs/azureml/job_prep_azureml.log\": \"https://deeplearstorage827de82c2.blob.core.windows.net/azureml/ExperimentRun/dcid.textgen_1611271774_5faca64f/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=V8xHlBZk4NMDX6yVhnSkaUeQGLu6dEVlEaJ5cIV7xmg%3D&st=2021-01-21T23%3A20%3A16Z&se=2021-01-22T07%3A30%3A16Z&sp=r\", \"logs/azureml/sidecar/tvmps_cff03917453a8001f928502ec2a6147c013beeb956d6b3167d39aa64a00686f6_p/all.log\": \"https://deeplearstorage827de82c2.blob.core.windows.net/azureml/ExperimentRun/dcid.textgen_1611271774_5faca64f/logs/azureml/sidecar/tvmps_cff03917453a8001f928502ec2a6147c013beeb956d6b3167d39aa64a00686f6_p/all.log?sv=2019-02-02&sr=b&sig=stBWghihqi7EdQgB2gg7ZTKAdd9Uy3mUqdlgXvKuMr8%3D&st=2021-01-21T23%3A20%3A16Z&se=2021-01-22T07%3A30%3A16Z&sp=r\", \"logs/azureml/sidecar/tvmps_cff03917453a8001f928502ec2a6147c013beeb956d6b3167d39aa64a00686f6_p/task.enter_contexts.log\": \"https://deeplearstorage827de82c2.blob.core.windows.net/azureml/ExperimentRun/dcid.textgen_1611271774_5faca64f/logs/azureml/sidecar/tvmps_cff03917453a8001f928502ec2a6147c013beeb956d6b3167d39aa64a00686f6_p/task.enter_contexts.log?sv=2019-02-02&sr=b&sig=hvYR%2BGYEPKGGTgEj%2FthtPBCyYTPZn9Zh2p9TapRiT8U%3D&st=2021-01-21T23%3A20%3A16Z&se=2021-01-22T07%3A30%3A16Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/process_info.json\", \"azureml-logs/process_status.json\", \"logs/azureml/dataprep/backgroundProcess.log\", \"logs/azureml/dataprep/backgroundProcess_Telemetry.log\", \"logs/azureml/job_prep_azureml.log\"], [\"logs/azureml/dataprep/engine_spans_02cf2465-81c4-462e-837f-a64fc8edff3b.jsonl\", \"logs/azureml/dataprep/python_span_02cf2465-81c4-462e-837f-a64fc8edff3b.jsonl\"], [\"azureml-logs/55_azureml-execution-tvmps_cff03917453a8001f928502ec2a6147c013beeb956d6b3167d39aa64a00686f6_p.txt\"], [\"logs/azureml/dataprep/engine_spans_65e52490-dfa0-4f1a-9482-e15c542a6721.jsonl\", \"logs/azureml/dataprep/python_span_65e52490-dfa0-4f1a-9482-e15c542a6721.jsonl\", \"azureml-logs/65_job_prep-tvmps_cff03917453a8001f928502ec2a6147c013beeb956d6b3167d39aa64a00686f6_p.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"logs/azureml/73_azureml.log\"], [\"logs/azureml/sidecar/tvmps_cff03917453a8001f928502ec2a6147c013beeb956d6b3167d39aa64a00686f6_p/all.log\", \"logs/azureml/sidecar/tvmps_cff03917453a8001f928502ec2a6147c013beeb956d6b3167d39aa64a00686f6_p/task.enter_contexts.log\"]], \"run_duration\": \"0:01:40\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"[2021-01-21T23:29:51.227060] DEBUG azureml.sidecar.try_add_control_script_to_path: Adding ES Control Scripts to sys.path: /mnt/batch/tasks/shared/LS_root/jobs/deep-learning/azureml/textgen_1611271774_5faca64f/mounts/workspaceblobstore/azureml/textgen_1611271774_5faca64f-setup\\n[2021-01-21T23:29:51.227935] DEBUG azureml.sidecar.TaskRegistry: Registering handler for task \\\"enter_contexts\\\"\\n[2021-01-21T23:29:51.228655] DEBUG azureml.sidecar.TaskRegistry: Registering handler for task \\\"exit_contexts\\\"\\n[2021-01-21T23:29:51.237543] DEBUG azureml.sidecar.set_run_wd: Changing working dir to: /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/deep-learning/azureml/textgen_1611271774_5faca64f/mounts/workspaceblobstore/azureml/textgen_1611271774_5faca64f\\n[2021-01-21T23:29:51.237922] INFO azureml.sidecar.sidecar: Received task: enter_contexts. Running on Linux at /mnt/hostfs/mnt/batch/tasks/shared/LS_root/jobs/deep-learning/azureml/textgen_1611271774_5faca64f/mounts/workspaceblobstore/azureml/textgen_1611271774_5faca64f\\n[2021-01-21T23:29:51.238273] INFO azureml.sidecar.sidecar: Invoking \\\"enter_contexts\\\" task with Context Managers: {\\\"context_managers\\\": [\\\"Dataset:context_managers.Datasets\\\"]}\\n[2021-01-21T23:29:51.238601] DEBUG azureml.sidecar.TaskRegistry.execute: Executing \\\"enter_contexts\\\"\\n[2021-01-21T23:29:51.238955] DEBUG azureml.sidecar.TaskServerClient.__init__: Connecting to TaskServer at 127.0.0.1:40505\\n[2021-01-21T23:29:51.239282] DEBUG azureml.sidecar.TaskRegistry.execute: Sending payload for \\\"enter_contexts\\\":\\n{\\\"host_secret\\\": \\\"ad53374a-e192-4b04-9592-1eb7ff117425\\\", \\\"task\\\": \\\"enter_contexts\\\", \\\"caller_session_id\\\": \\\"654f9ce0-9c6c-4a0d-b51d-bda920126c2f\\\", \\\"context_managers\\\": [\\\"Dataset:context_managers.Datasets\\\"], \\\"job_task_error_path\\\": \\\"/mnt/batch/tasks/workitems/0d59db81-fc84-468c-bb81-378c1aa3572a/job-1/textgen_1611271774_5_20385a31-9728-4e9e-bd95-cdc2a489c889/wd/runSpecialJobTask_error.json\\\"}\\n[2021-01-21T23:30:00.305668] DEBUG azureml.sidecar.TaskRegistry.execute: Received response for \\\"enter_contexts\\\": {\\\"result\\\": \\\"success\\\"}\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.19.0\"}, \"loading\": false}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 35,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611271778794
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 36,
          "data": {
            "text/plain": "Run(Experiment: textgen,\nId: textgen_1611271774_5faca64f,\nType: azureml.scriptrun,\nStatus: Queued)",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>textgen</td><td>textgen_1611271774_5faca64f</td><td>azureml.scriptrun</td><td>Queued</td><td><a href=\"https://ml.azure.com/experiments/textgen/runs/textgen_1611271774_5faca64f?wsid=/subscriptions/fc814e44-1cd5-4ab5-944b-f2255f816d34/resourcegroups/text-generation/workspaces/deep-learning\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 36,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611271780158
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RunId: textgen_1611271774_5faca64f\n",
            "Web View: https://ml.azure.com/experiments/textgen/runs/textgen_1611271774_5faca64f?wsid=/subscriptions/fc814e44-1cd5-4ab5-944b-f2255f816d34/resourcegroups/text-generation/workspaces/deep-learning\n",
            "\n",
            "Streaming azureml-logs/55_azureml-execution-tvmps_cff03917453a8001f928502ec2a6147c013beeb956d6b3167d39aa64a00686f6_p.txt\n",
            "========================================================================================================================\n",
            "\n",
            "2021-01-21T23:29:44Z Starting output-watcher...\n",
            "2021-01-21T23:29:44Z IsDedicatedCompute == False, starting polling for Low-Pri Preemption\n",
            "2021-01-21T23:29:45Z Executing 'Copy ACR Details file' on 10.0.0.5\n",
            "2021-01-21T23:29:45Z Copy ACR Details file succeeded on 10.0.0.5. Output: \n",
            ">>>   \n",
            ">>>   \n",
            "Login Succeeded\n",
            "Using default tag: latest\n",
            "latest: Pulling from azureml/azureml_9d4fa30783fc98f2c7c7f19c6a312f30\n",
            "Digest: sha256:96a223a2d683aab4b4f91719ba3f705a79883c430ca39e73845fd2ba36704f14\n",
            "Status: Image is up to date for viennaglobal.azurecr.io/azureml/azureml_9d4fa30783fc98f2c7c7f19c6a312f30:latest\n",
            "viennaglobal.azurecr.io/azureml/azureml_9d4fa30783fc98f2c7c7f19c6a312f30:latest\n",
            "2021-01-21T23:29:46Z Check if container textgen_1611271774_5faca64f already exist exited with 0, \n",
            "\n",
            "c3d600f86e7fc48308c6da135336ab9304a6f6f40e30411f63ec5a4af5e9ce52\n",
            "2021/01/21 23:29:47 Starting App Insight Logger for task:  containerSetup\n",
            "2021/01/21 23:29:47 Version: 3.0.01467.0023 Branch: .SourceBranch Commit: 8f9022f\n",
            "2021/01/21 23:29:47 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
            "2021/01/21 23:29:47 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
            "2021/01/21 23:29:47 sshd inside container not required for job, skipping setup.\n",
            "2021/01/21 23:29:48 All App Insights Logs was send successfully\n",
            "2021-01-21T23:29:48Z Starting docker container succeeded.\n",
            "\n",
            "Streaming azureml-logs/70_driver_log.txt\n",
            "========================================\n",
            "\n",
            "bash: /azureml-envs/azureml_503a0f9c4b6b1e454b46bfa74eb4f8b7/lib/libtinfo.so.5: no version information available (required by bash)\n",
            "bash: /azureml-envs/azureml_503a0f9c4b6b1e454b46bfa74eb4f8b7/lib/libtinfo.so.5: no version information available (required by bash)\n",
            "2021/01/21 23:30:04 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/info\n",
            "2021/01/21 23:30:05 Attempt 1 of http call to http://10.0.0.5:16384/sendlogstoartifacts/status\n",
            "[2021-01-21T23:30:06.272382] Entering context manager injector.\n",
            "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'Dataset:context_managers.Datasets', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['script.py', '--data-folder', 'DatasetConsumptionConfig:text', '--text-id', '1', '--text-size', '10000', '--epochs', '1'])\n",
            "Script type = None\n",
            "Starting the daemon thread to refresh tokens in background for process with pid = 73\n",
            "Entering Run History Context Manager.\n",
            "[2021-01-21T23:30:09.103501] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/deep-learning/azureml/textgen_1611271774_5faca64f/mounts/workspaceblobstore/azureml/textgen_1611271774_5faca64f\n",
            "[2021-01-21T23:30:09.103605] Preparing to call script [script.py] with arguments:['--data-folder', '$text', '--text-id', '1', '--text-size', '10000', '--epochs', '1']\n",
            "[2021-01-21T23:30:09.103679] After variable expansion, calling script [script.py] with arguments:['--data-folder', '/mnt/batch/tasks/shared/LS_root/jobs/deep-learning/azureml/textgen_1611271774_5faca64f/wd/tmpdi268icm', '--text-id', '1', '--text-size', '10000', '--epochs', '1']\n",
            "\n",
            "2021-01-21 23:30:09.207598: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_503a0f9c4b6b1e454b46bfa74eb4f8b7/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\n",
            "2021-01-21 23:30:09.207718: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "training dataset is stored here: /mnt/batch/tasks/shared/LS_root/jobs/deep-learning/azureml/textgen_1611271774_5faca64f/wd/tmpdi268icm\n",
            "---------- Wczytywanie danych ----------\n",
            "Length of text corpus_text_12000000.txt: 465558088 characters\n",
            "Length of text corpus_text_16000000.txt: 449158961 characters\n",
            "Length of text corpus_text_20000000.txt: 482053012 characters\n",
            "Length of text corpus_text_24000000.txt: 447787141 characters\n",
            "Length of text corpus_text_28000000.txt: 492869655 characters\n",
            "Length of text corpus_text_32000000.txt: 426814958 characters\n",
            "Length of text corpus_text_36000000.txt: 376931846 characters\n",
            "Length of text corpus_text_4000000.txt: 507616439 characters\n",
            "Length of text corpus_text_40000000.txt: 115429126 characters\n",
            "Length of text corpus_text_8000000.txt: 480254666 characters\n",
            "---------- ZakoÅ„czono wczytywanie danych ----------\n",
            "---------- Przetwarzanie tekstu 1/2----------\n",
            "--------- 0/1 ----------\n"
          ]
        }
      ],
      "execution_count": 37,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611266818293
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.get_details()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611263299995
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.get_metrics()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611266818596
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.get_file_names()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611266818826
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a model folder in the current directory\r\n",
        "os.makedirs('./model', exist_ok=True)\r\n",
        "\r\n",
        "for f in run.get_file_names():\r\n",
        "    if f.startswith('outputs/'):\r\n",
        "        print(f.replace('outputs/model/', \"\"))\r\n",
        "        output_path = os.path.join('./model/', f.replace('outputs/model/', \"\"))\r\n",
        "        print('Downloading from {} to {} ...'.format(f, output_path))\r\n",
        "        run.download_file(name=f, output_file_path=output_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1611266819114
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}